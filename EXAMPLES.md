# 使用示例

本文档提供 LLExtTool 的使用场景和示例。

## 基本使用流程

### 场景 1: 提取日语视频字幕并翻译成中文

**适用场景**: 日本动画、日剧、日语教学视频等

**步骤**:

1. **启动应用**
   ```powershell
   npm start
   ```

2. **配置设置** (首次使用)
   - 点击右上角 "⚙️ 设置"
   - Whisper 模型路径: `F:\GitProject\LLExtTool\models\whisper\ggml-base.bin`
   - 输出目录: `F:\Videos\Subtitles`
   - 点击 "保存"

3. **选择视频**
   - 点击 "选择" 按钮
   - 选择视频文件，例如: `anime-episode-01.mp4`
   - 视频信息会自动显示（时长、分辨率等）

4. **设置参数**
   - 源语言: 选择 "日语"
   - 目标语言: "中文" (默认)
   - 音频格式: "WAV" (推荐)

5. **开始处理**
   - 点击 "开始处理" 按钮
   - 等待处理完成（进度条显示）
   - 处理时间取决于视频长度和电脑性能

6. **编辑字幕**
   - 在右侧字幕编辑器中查看识别结果
   - 修改错误的识别文本
   - 修正翻译错误
   - 为每句字幕分配说话人

7. **导出字幕**
   - 点击 "💾 导出字幕"
   - 选择保存位置和格式（SRT/VTT/JSON）
   - 字幕文件已生成

**预期结果**:
- 音频提取: 约 10 秒（1分钟视频）
- 语音识别: 约 20-40 秒（使用 base 模型）
- 翻译: 约 5-10 秒
- 总时间: 约 35-60 秒（1分钟视频）

---

### 场景 2: 英语视频添加中文字幕

**适用场景**: 英语电影、TED 演讲、教程视频等

**步骤**:

1. 选择英语视频文件
2. 源语言选择 "英语"
3. 其他步骤同场景 1

**提示**:
- 英语识别通常比日语更准确
- Whisper 对英语的支持最好
- 可以使用更小的模型（tiny/base）也能获得好效果

---

### 场景 3: 仅转录不翻译

**适用场景**: 会议记录、采访、播客等

**步骤**:

1. 选择音频/视频文件
2. 选择源语言
3. 处理完成后，导出时选择仅包含原文

**技巧**:
- 对于长时间录音，使用 base 模型以平衡速度和准确度
- 如果是会议记录，可以使用说话人分类功能标记不同发言人

---

### 场景 4: 批量处理多个视频

**当前版本暂不支持批量处理，但可以通过脚本实现：**

创建 `batch-process.js`:

```javascript
const { spawn } = require('child_process');
const fs = require('fs');
const path = require('path');

const videoDir = 'F:\\Videos\\ToProcess';
const videos = fs.readdirSync(videoDir).filter(f => f.endsWith('.mp4'));

async function processVideo(videoPath) {
  // 这里需要通过 IPC 或 CLI 接口调用处理
  // 当前版本需要手动处理
  console.log(`Processing: ${videoPath}`);
}

videos.forEach(processVideo);
```

---

## 高级用法

### 自定义说话人标签

在字幕编辑器中：

1. 点击 "➕ 添加说话人"
2. 输入说话人名称，例如：
   - "旁白"
   - "主角-张三"
   - "配角-李四"
3. 在每个字幕项中选择说话人

导出时可以选择包含说话人信息，生成带说话人标签的字幕。

### 手动修正时间轴

虽然 Whisper 生成的时间轴通常很准确，但有时需要微调：

1. 在字幕列表中找到需要调整的项
2. 修改文本内容
3. 如需调整时间，可以记录时间戳
4. 导出为 JSON 格式
5. 用文本编辑器手动调整时间
6. 重新导入（功能待实现）

### 多语言字幕

创建双语字幕文件：

1. 导出时选择 "包含原文" 和 "包含翻译"
2. 生成的 SRT 文件会包含两行文本
3. 大多数视频播放器支持这种格式

示例输出：
```
1
00:00:01,000 --> 00:00:03,500
こんにちは、みなさん！
你好，大家！

2
00:00:03,500 --> 00:00:06,000
今日はいい天気ですね。
今天天气真好啊。
```

---

## 实际案例

### 案例 1: 动画片头曲字幕制作

**任务**: 为动画 OP 制作双语字幕

**视频**: 90 秒动画片头
**歌词特点**: 日语，演唱速度快

**处理步骤**:
1. 使用 small 模型以提高准确度
2. 处理时间: 约 2 分钟
3. 识别准确率: ~85%
4. 手动修正歌词: 约 5 分钟
5. 翻译质量: 需要人工润色
6. 总耗时: 约 10 分钟

**经验**:
- 歌曲识别比对话难，建议使用更大的模型
- 快节奏的歌曲可能需要更多手动修正
- 翻译歌词建议保持原意同时注意押韵

### 案例 2: 教学视频字幕

**任务**: 为编程教程添加字幕

**视频**: 45 分钟 Python 教程（英语）
**特点**: 清晰发音，技术术语多

**处理步骤**:
1. 使用 base 模型
2. 处理时间: 约 15 分钟
3. 识别准确率: ~95%
4. 技术术语需要少量修正
5. 翻译专业术语时需要注意
6. 总耗时: 约 30 分钟

**经验**:
- 清晰的英语识别效果很好
- 技术术语建议保留英文或添加注释
- 可以使用代码格式标记专有名词

### 案例 3: 会议记录转写

**任务**: 将 2 小时会议录音转为文字

**音频**: MP3 格式，中等音质
**特点**: 多人讨论，有背景噪音

**处理步骤**:
1. 先用 FFmpeg 提取音频: 约 30 秒
2. 使用 small 模型提高准确率
3. 处理时间: 约 40 分钟
4. 使用说话人分类标记发言人
5. 修正错误和整理格式: 约 1 小时
6. 总耗时: 约 1.5 小时

**经验**:
- 背景噪音会降低准确率
- 建议录音前测试麦克风质量
- 多人对话需要仔细标记说话人

---

## 性能参考

基于不同硬件和模型的处理时间（1分钟视频）：

| 配置 | 模型 | 提取音频 | 识别 | 翻译 | 总计 |
|-----|------|---------|------|------|------|
| i5-8400, 16GB | base | 10s | 30s | 5s | 45s |
| i7-10700, 32GB | small | 8s | 50s | 5s | 63s |
| i9-12900K, 64GB | medium | 6s | 80s | 4s | 90s |
| Ryzen 7 5800X | base | 9s | 28s | 5s | 42s |

**注意**: 
- 实际时间会根据视频内容、音频质量等因素变化
- GPU 加速版本（待实现）可以显著提高识别速度
- 第一次运行会加载模型，会比较慢

---

## 技巧和最佳实践

### 提高识别准确率

1. **选择合适的模型**
   - 日常使用: base
   - 专业需求: small/medium
   - 测试开发: tiny

2. **优化音频质量**
   - 降噪处理
   - 标准化音量
   - 使用 WAV 格式

3. **选择正确的语言**
   - 确保语言设置正确
   - 混合语言的视频可能需要分段处理

### 提高翻译质量

1. **后处理翻译结果**
   - 人工检查专业术语
   - 润色语句通顺度
   - 保持原文语气

2. **使用更好的翻译模型**
   - 尝试 NLLB 模型
   - 考虑使用 DeepL API
   - 对于重要项目，使用专业翻译

### 优化工作流程

1. **批量处理**
   - 先处理所有视频
   - 统一编辑和审核
   - 批量导出

2. **团队协作**
   - 导出 JSON 格式
   - 使用版本控制
   - 分工处理不同部分

3. **建立模板**
   - 保存常用说话人列表
   - 记录常见术语翻译
   - 使用一致的格式规范

---

## 常见问题案例

### 问题 1: 识别的文本有大量错误

**可能原因**:
- 音频质量差（背景噪音大）
- 说话不清晰或口音重
- 使用的模型太小

**解决方案**:
- 使用音频降噪软件预处理
- 尝试使用更大的模型（small/medium）
- 手动修正错误部分

### 问题 2: 翻译结果不准确

**可能原因**:
- 专业术语或俚语
- 文化相关内容
- 模型限制

**解决方案**:
- 使用在线翻译 API（DeepL）
- 人工润色重要部分
- 建立术语表

### 问题 3: 处理速度太慢

**可能原因**:
- 硬件性能不足
- 模型太大
- 视频太长

**解决方案**:
- 使用更小的模型（tiny/base）
- 分段处理长视频
- 升级硬件或等待 GPU 加速版本

---

## 下一步

探索更多功能：
- 查看 `DEVELOPMENT.md` 了解开发细节
- 查看 `TRANSLATION_GUIDE.md` 配置翻译
- 参与项目开发，贡献新功能

享受使用 LLExtTool！
